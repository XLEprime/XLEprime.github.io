<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>DANNet论文笔记 | __XLE的博客</title><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script src="https://unpkg.com/mermaid@8.13.5/dist/mermaid.min.js"></script><script>mermaid.initialize({
  startOnLoad: true
  , theme: 'dark'
});</script><link rel="stylesheet" href="/css/arknights.css"><link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/highlight.js/10.1.2/styles/atom-one-dark-reasonable.min.css"><style>@font-face {
 font-family: BenderLight;
 src: local('Bender'), url("/font/BenderLight.ttf");
}
@font-face {
 font-family: 'JetBrains Mono';
 src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}</style><meta name="generator" content="Hexo 6.0.0"></head><body style="background-image:url(https://ak.hypergryph.com/assets/index/images/ak/pc/bk.jpg);"><header><nav><a href="/">Home</a><a href="/archives/">Archives</a></nav></header><main><article><div id="post-bg"><div id="post-title"><h1>DANNet论文笔记</h1><hr></div><div id="post-content"><p>#DANNet论文笔记</p>
<p>##前言<br><strong>DANNet</strong>是发表在<strong>CVPR 2021</strong>的paper，是第一个用于夜间语义分割的单阶段适应(one-stage adaptation)框架，很有精读的意义。本来这篇博客title应该写为论文精读，但是我懒得把所有内容都写进来，故改为笔记。<br>（<strong>DANNet</strong>不要和<strong>DANet</strong>弄混了）</p>
<p>##施工进度<br>只写了文字部分，在markdown里敲latex属实是折磨自己。图床也没学会咋用，属于是废物一个。（其实还没学）</p>
<p>##论文链接<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.10834">DANNet</a></p>
<p>##讲解参考<br>暂无。夜间图像语义分割这块做的人有点少。</p>
<p>##主要贡献</p>
<ol>
<li>DANNNet 是第一个用于夜间语义分割的单阶段适应(one-stage adaptation)框架。这个域适应的过程是通过GAN实现的，设计一个共享权重的RelightNet和Semantic Segmentation Network充当GAN中的Generator，然后有两个Discriminators分别识别来自源域$S$(数据来自ImageNet)还是目标域$D_d$(Dark Zurich-D (daytime))和识别来自源域$S$(数据来自ImageNet)还是目标域$D_n$(Dark Zurich-N (nighttime))的。</li>
<li>用daytime图像分割的结果作为对应nighttime的label来作伪监督学习，因为拍摄时间不同，只能保证静态物体(比如天空，楼房，街道等)的位置信息相同，所以此处计算Loss的时候只考虑静态对象的分类。</li>
<li>Ablation study证明了DANNet的每一个部件都是有效的。（我看完觉得消融实验做的不完全，精读后发现很多部件的有效性实际上没有证明。）</li>
</ol>
<p>##三个模块<br>DANNet由RelightNet，Segmentation Network和GAN三个部分构成。</p>
<p>###RelightNet</p>
<ol>
<li>RelightNet的作用是让来源不同的三类输入在进入分割网络前的强度分布相互靠近，也就是$𝑅<em>𝑠$、$𝑅</em>{𝑡𝑑}$和$𝑅_{𝑡𝑛}$在强度分布上要相似。这个网络一共有三个$Loss$组成了$𝐿_{𝑙𝑖𝑔ℎ𝑡}$，也就是$L_{light}=α_{𝑡𝑣}𝐿_{𝑡𝑣}+α_{𝑒𝑥𝑝}𝐿_{𝑒𝑥𝑝}+α_{𝑠𝑠𝑖𝑚}L_{𝑠𝑠𝑖𝑚}$。<br>$$L_{tv}=\frac{1}{N}|(\nabla_{x}(I-R))^{2}+(\nabla_{y}(I-R))^{2}|_{1} \tag{3}$$</li>
<li>其中$𝐿_{𝑡𝑣}$是一种广泛应用于图像去噪和图像合成的损失函数，目的是让生成的图像噪点更小，看起来更加平滑，有利于之后的分割网络。</li>
<li>其中$𝐿_{𝑒𝑥𝑝}$ 用来让白天和黑夜场景下的输入最终有相近的光照效果。</li>
<li>其中$L_{𝑠𝑠𝑖𝑚}$是一种广泛应用在图像重建的损失函数，是为了保证生成的重光照结果R和输入I有着相同的结构信息。此处使用简化版本的SSIM。</li>
</ol>
<p>###Segmentation Network</p>
<ol>
<li>语义分割网络采用了三种热门的网络，分别是Deeplab-v2，RefineNet和PSPNet ，backbone全部采用ResNet-101。</li>
<li>一次训练中只使用上述的一种网络，对于来自三个域的输入，共享网络权重。三个域输入分割的结果记作$𝑅<em>𝑠$、$𝑅</em>{𝑡𝑑}$和$𝑅_{𝑡𝑛}$。</li>
<li>由于源域中不同对象类别的像素数量不平衡，占比大的类别(比如sky, road and sidewalk等)容易收敛，而占比小的类别则相反。统计数据集中每一个对象类别的像素占比，设计一个re-weighting策略。避免数值爆炸使用了对数，并且统计了平均值和标准差对重加权的参数进行标准化（训练时凭经验初始化了std=0.05，avg=1.0，但在消融实验中std=0.16时效果最好）。</li>
<li>结合(3.)中提到的重加权策略改进了cross-entropy loss，记作$𝐿_{𝑠𝑒𝑔}$。注意只有源域的输入有GT，所以只有源域进行这个损失函数计算。</li>
<li>来自daytime和nighttime的分割结果可以进行伪监督学习，即是用daytime图像分割的结果作为对应nighttime的label，然后进行损失函数的计算。这里只考虑静态目标类别，损失函数记作$𝐿_{𝑠𝑡𝑎𝑡𝑖𝑐}$。因为两张图拍摄时可能没有完全对其，所以引入𝑝(𝑐,𝑖)来解决这个问题。公式(9)中的j表示i周围$3\times3$的坐标，c表示目标类别。</li>
</ol>
<p>###GAN</p>
<ol>
<li>Generator $G$ 由重光照网络和语义分割网络组成。</li>
<li>有两个Discriminators，记作$𝐷_𝑑$和$𝐷_𝑛$，分别识别来自源域$𝑆$(数据来自CityScapes，有label)还是目标域$𝑇_𝑑$(Dark Zurich-D (daytime))和识别来自源域$S$(数据来自CityScapes，有label)还是目标域$𝑇_𝑛$(Dark Zurich-N (nighttime))的。</li>
<li>把前两个网络封装看作$G$，$G$和$D$交替训练。</li>
<li>𝐺训练时的损失函数为$𝐿_{𝑡𝑜𝑡𝑎𝑙}$，为了使得分割结果$𝑃_{𝑡𝑑}$和$𝑃_{𝑡𝑛}$与源域的分割结果$𝑃<em>𝑠$相近，增加了损失函数$𝐿</em>{𝑎𝑑𝑣}$，本质是两个最小二乘损失函数的和。其中$r$是$𝑃_{𝑡𝑑}$和$𝑃_{𝑡𝑛}$对应的label。</li>
<li>𝐷训练时的损失函数为$𝐿_𝑑$和$𝐿_𝑛$，其中符号定义与(4.)中类似。</li>
</ol>
<p>##可以改进的点<br>等我改进成功了再写吧</p>
<div id="paginator"></div></div><div id="post-footer"><hr><a href="/2020/01/07/hello/">Hello! Next →</a><hr></div><div id="bottom-btn"><a id="to-top" href="#post-title" title="to top">∧</a></div><div id="Valine"></div><script>new Valine({
 el: '#Valine'
 , appId: 'UwTw9PNim8W2wkLamexajaKY-gzGzoHsz'
 , appKey: 'claRFYd64AIp4FYKA72z0FVD'
 , placeholder: '此条评论委托企鹅物流发送'
})</script></div></article><aside><div id="about"><a href="/" id="logo"><img src="https://ak.hypergryph.com/assets/index/images/ak/pc/faction/1.png" alt="Logo"></a><h1 id="Dr"><a href="/"> Dr.Xleprime</a></h1><section id="total"><a id="total-archives" href="/archives"><span class="total-title">Archives Total:</span><span class="total-number">2</span></a><div id="total-tags"><span class="total-title">Tags:</span><span class="total-number">5</span></div><div id="total-categories"><span class="total-title">Categories:</span><span class="total-number">2</span></div></section></div><div id="aside-block"></div><footer><nobr><span class="text-title">©</span><span class="text-content"></span></nobr><wbr><nobr><span class="text-title">ICP</span><span class="text-content"></span></nobr><wbr><wbr><nobr>published with&nbsp;<a target="_blank" rel="noopener" href="http://hexo.io">Hexo&nbsp;</a></nobr><wbr><nobr>Theme&nbsp;<a target="_blank" rel="noopener" href="https://github.com/Yue-plus/hexo-theme-arknights">Arknight&nbsp;</a></nobr><wbr><nobr>by&nbsp;<a target="_blank" rel="noopener" href="https://github.com/Yue-plus">Yue_plus</a></nobr></footer></aside></main><canvas id="canvas-dust"></canvas><script src="/js/arknights.js"></script><script src="https://cdn.bootcdn.net/ajax/libs/highlight.js/10.1.2/highlight.min.js"></script></body></html>